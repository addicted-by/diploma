Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop


yes \bibitem{Fang} Xing Fang, Justin Zhan, Sentiment analysis using product review data;
yes    \bibitem{Kotelnikov} Котельников Е.В., Клековкина М.В.,
                 Автоматический анализ тональности текстов на основе методов машинного обучения;
yes    \bibitem{Ghorbari}  Monshen Ghorbari, Mahdi Bahaghighat,
                 Qin Xinm Figen Ozen, ConvLSTMConv network: a deep learning approach for sentiment analysis in cloud computing;
yes    \bibitem{Xing}  Yongping Xing, Chuangbai Xiao, A GRU Model for Aspect Level Sentiment Analysis;
yes    \bibitem{LeCun 1995}	Y. LeCun and Y.Bengio, 
                Convolutional networks for images, speech and time-series. Brain Theory and Neural Networks, 1995;
yes    \bibitem{LeCun 2010}    Y. LeCun, K. Kavukcuoglu and C. Farabet. Convolutional networks and applica-tions in vision. 
                In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE Inter-national Symposium on, pages 253-256. IEEE, 2010;
yes    \bibitem{LeCun 1990}	Y. LeCun, B.Boser, J.S. Denker, D. Henderson, R.E.Howard, W. Hubbard, L.D. Jackel, et al. 
                Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, 1990;
 yes   \bibitem{Goodfellow}	"Deep learning” by Ian Goodfellow, Yoshua Bengio, Aaron Courville, pages 283-308, 2017.
 yes   \bibitem{Zhou}  Zhou, Y. and Chellappa R. Computation of optical flow using a neural network. In Neural Networks, 1988.
yes    \bibitem{Szegedy2014}   Szegedy, C., Liu, W., Jia, Y., Sermanet, P. Reed, S. Anguelov, D. Erhan, D. Vanhoucke, V. and Rabinovich A. Going deeper with convolutions. Technical report, 2014; 
 yes   \bibitem{Srivastava}	Srivastava N., Hinton G., Krizhevsky A, Sutskever I., Salakjutdinov R. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 2014, 15, 1929-1958;
 yes   \bibitem{Schmidhuber2015}   J. Schmidhuber, “Deep learning in neural networks: an overview”, Neural networks, vol. 61, pp. 29-33, 2015;
 yes   \bibitem{Schmidhuber1996}   Sepp Honchreiter, Jurden Shmidhuber, Long Short-Term Memory. Neural Comput 1996; 9 (8): 1735-1780;
 yes   \bibitem{Zipser}	R. J. Williams and D. Zipser. Gradient-based Learning Algorithms for Recurrent Networks and Their Computational Complexity. In Y. Chauvin and D. E. Rumelhart, editors, Back-propogation: Theory, Architectures and Applications, pages 433-486. Lawrence Erlbaum Publishers, 1995;
    \bibitem{Williams}  R. J. Williams and D. Zipser. Gradient-based Learning Algorithms for Recurrent Networks and Their Computational Complexity. In Y. Chauvin and D. E. Ru-melhart, editors, Back-propogation: Theory, Architectures and Applications, pages 433-486. Lawrence Erlbaum Publishers, 1995;
    \bibitem{Schmidhuber2001}   S. Hochreiter. Y. Bengio, P. Frasconi and J. Schmidhuber. Gradient Flow in Recur-rent Nets: the Difficulty of Learning Long-term Dependencies. In S.C. Kremer and J.F. Kolen, editors, A Field Guide to Dynamical Recurrent Networks. IEEE Press, 2001a; 
    \bibitem{Sutskever}Jozefowicz R., Zaremba W. and Sutskever I. An empirical evaluation of recurrent network architectures. In ICML2015, 2015;
@article{Fang,
author = {Fang, Xing and Zhan, Justin},
year = {2015},
month = {12},
pages = {},
title = {Sentiment analysis using product review data},
volume = {2},
journal = {J Big Data},
doi = {10.1186/s40537-015-0015-2}
}
@inproceedings{LeCun:1,
author = {Lecun, Yann and Bengio, Y.},
year = {1995},
month = {01},
pages = {},
title = {Convolutional Networks for Images, Speech, and Time-Series},
journal = {The Handbook of Brain Theory and Neural Networks}
}
@inproceedings{LeCun:2,
author = {Lecun, Yann and Kavukcuoglu, Koray and Farabet, Clement},
year = {2010},
month = {05},
pages = {253-256},
title = {Convolutional Networks and Applications in Vision},
journal = {ISCAS 2010 - 2010 IEEE International Symposium on Circuits and Systems: Nano-Bio Circuit Fabrics and Systems},
doi = {10.1109/ISCAS.2010.5537907}
}
@inproceedings{LeCun:3,
author = {Le Cun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
title = {Handwritten Digit Recognition with a Back-Propagation Network},
year = {1989},
publisher = {MIT Press},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {396–404},
numpages = {9},
series = {NIPS'89}
}
@inproceedings{Kotelnikov:1,
  author          = {Котельников, Е. В. and Клековкина, М. В.},
  organization       = {Компьютерная лингвистика и интеллектуальные технологии},
  title           = {Автоматический анализ тональности текстов на основе методов машинного обучения},
  year            = {2012},
  publisher = {РГГУ},
  pages = {27-36}
}
@article{gagar,
	title        = {{Детекция оружия в реальном времени нейросетевыми методами для задач обеспечения безопасности и разведки}},
    author       = {Рябыкин, А. С. and Сухов, Е. А.},
	year         = 2022,
	journal      = {XLVIII Международная молодёжная научная конференция «Гагаринские чтения – 2022»},
	publisher    = {Перо}
}
@article{Xing:1,
author = {Xing, Yongping and Xiao, Chuangbai},
year = {2019},
month = {08},
pages = {032042},
title = {A GRU Model for Aspect Level Sentiment Analysis},
volume = {1302},
journal = {Journal of Physics: Conference Series},
doi = {10.1088/1742-6596/1302/3/032042}
}
@article{Ghorbani:1,
author = {Ghorbani, Mohsen and Bahaghighat, Mahdi and Xin, Qin and Özen, Figen},
year = {2020},
month = {03},
pages = {16},
title = {ConvLSTMConv network: a deep learning approach for sentiment analysis in cloud computing},
volume = {9},
journal = {Journal of Cloud Computing Advances Systems and Applications},
doi = {10.1186/s13677-020-00162-1}
}
"Deep learning” by Ian Goodfellow, Yoshua Bengio, Aaron Courville, pages 283-308, 2017.
@book{Goodfellow,
  author         = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  title          = {Deep learning},
  year           = {2017},
  pages         = {283-308}
}
@inproceedings{Zhou:1,
author = {Zhou, Y.T. and Chellappa, Rama},
year = {1988},
month = {08},
pages = {71 - 78 vol.2},
title = {Computation of optical flow using a neural network},
doi = {10.1109/ICNN.1988.23914}
}
@article{Szegedy2014,
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
year = {2014},
month = {09},
pages = {},
title = {Going Deeper with Convolutions}
}
@article{Srivastava:1,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}
@article{Schmidhuber2015,
title = {Deep learning in neural networks: An overview},
journal = {Neural Networks},
volume = {61},
pages = {85-117},
year = {2015},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014002135},
author = {Jürgen Schmidhuber}
}
@article{harris54,
  added-at = {2020-05-20T16:56:27.000+0200},
  author = {Harris, Zellig},
  biburl = {https://www.bibsonomy.org/bibtex/252e7950c31610617170d71c320f2252e/ghagerer},
  doi = {10.1007/978-94-009-8467-7_1},
  interhash = {a23596808b6273076e1259dedca16330},
  intrahash = {52e7950c31610617170d71c320f2252e},
  journal = {Word},
  keywords = {bag-of-words},
  number = {2-3},
  pages = {146--162},
  publisher = {Taylor \& Francis},
  timestamp = {2020-06-24T14:53:20.000+0200},
  title = {Distributional structure},
  url = {https://link.springer.com/chapter/10.1007/978-94-009-8467-7_1},
  volume = 10,
  year = 1954
}
@article{curse,
author = {Venkat, Naveen},
year = {2018},
month = {09},
pages = {},
title = {The Curse of Dimensionality: Inside Out},
doi = {10.13140/RG.2.2.29631.36006}
}
@article{Schmidhuber:1996,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
journal = {Neural Comput.},
month = {nov},
pages = {1735–1780},
numpages = {46}
}
@MISC{Zipser:1,
    author = {Ronald J. Williams and David Zipser},
    title = {Gradient-Based Learning Algorithms for Recurrent Networks and Their Computational Complexity},
    year = {1995}
}
@misc{mikolov2013distributed,
      title={Distributed Representations of Words and Phrases and their Compositionality}, 
      author={Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1310.4546},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{cbow,
  doi = {10.48550/ARXIV.1301.3781},
  url = {https://arxiv.org/abs/1301.3781},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Efficient Estimation of Word Representations in Vector Space},
  publisher = {arXiv},
  year = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{skipgram,
  doi = {10.48550/ARXIV.1402.3722},
  url = {https://arxiv.org/abs/1402.3722},
  author = {Goldberg, Yoav and Levy, Omer},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{Schmidhuber2001,
author = {Informatik, Fakultit and Bengio, Y. and Frasconi, Paolo and Schmidhuber, Jfirgen},
year = {2003},
month = {03},
pages = {},
title = {Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies},
journal = {A Field Guide to Dynamical Recurrent Neural Networks}
}
@inproceedings{Sutskever:1,
author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
title = {An Empirical Exploration of Recurrent Network Architectures},
year = {2015},
publisher = {JMLR.org},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {2342–2350},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}
@article{hornik,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}
@article{barron,
author = {Barron, Andrew},
year = {1993},
month = {06},
pages = {930 - 945},
title = {Barron, A.E.: Universal approximation bounds for superpositions of a sigmoidal function. IEEE Trans. on Information Theory 39, 930-945},
volume = {39},
journal = {Information Theory, IEEE Transactions on},
doi = {10.1109/18.256500}
}
@misc{bert,
  doi = {10.48550/ARXIV.1810.04805},
  url = {https://arxiv.org/abs/1810.04805},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{backpropagation,
title = {Back-propagation algorithm which varies the number of hidden units},
journal = {Neural Networks},
volume = {4},
number = {1},
pages = {61-66},
year = {1991},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(91)90032-Z},
url = {https://www.sciencedirect.com/science/article/pii/089360809190032Z},
author = {Yoshio Hirose and Koichi Yamashita and Shimpei Hijiya},
keywords = {Neural networks, back-propagation, hidden unit, local minimum, weight},
abstract = {This report presents a back-propagation algorithm that varies the number of hidden units. This algorithm is expected to escape local minima and makes it no longer necessary to decide the number of hidden units. We tested this algorithm on two examples. One was exclusive-OR learning and the other was 8 × 8 dot alphanumeric font learning. In both examples, the probability of becoming trapped in local minima was reduced. Furthermore, in alphanumeric font learning, the network converged two to three times faster than conventional back-propagation.}
}
@misc{leland,
  doi = {10.48550/ARXIV.1802.03426},
  url = {https://arxiv.org/abs/1802.03426},
  author = {McInnes, Leland and Healy, John and Melville, James},
  keywords = {Machine Learning (stat.ML), Computational Geometry (cs.CG), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@book{clustering,
  title={Algorithms for clustering data},
  author={Jain, Anil K and Dubes, Richard C},
  year={1988},
  publisher={Prentice-Hall, Inc.}
}